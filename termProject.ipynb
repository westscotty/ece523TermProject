{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Project\n",
    "\n",
    "Team: Rain Price, Weston Scott\n",
    "\n",
    "ECE 523 | Engineering Applications of Machine Learning and Data Analytics\n",
    "\n",
    "Professor Abhijit Mahalanobis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle and People Detection with the FLIR Thermal Dataset\n",
    "\n",
    "![alt text](problemStatement.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Get simple training model working\n",
    "- Get resulting images and predictions showing\n",
    "- Run all images through model and check error\n",
    "- Rewrite Resnet layers to be our own homegrown solution?\n",
    "- Survive this class ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from PIL import Image, ImageFilter\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xml.etree.ElementTree as ET\n",
    "from torchvision.transforms.functional import pad\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import ToTensor, transforms\n",
    "from copy import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to see if input images are the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/images_thermal_train/data'\n",
    "\n",
    "imgs = os.listdir(path)\n",
    "sizes = []\n",
    "for img in imgs:\n",
    "    filename = os.path.join(path, img)\n",
    "    image = Image.open(filename)\n",
    "    sizes.append(image.size)\n",
    "print(sizes[-1])\n",
    "np.unique(sizes, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Seed, Device Architecture, and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "randomSeed = 2024\n",
    "np.random.seed(randomSeed)\n",
    "torch.manual_seed(randomSeed)\n",
    "\n",
    "print(f'PyTorch Version: {torch.__version__}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print(f'PyTorch Device: {device}')\n",
    "\n",
    "saveModel = True\n",
    "loadModel = False\n",
    "modelPath = './model.pt'\n",
    "numWorkers = 1 \n",
    "learnRate = 0.006 #2e-5\n",
    "batchSize = 1\n",
    "maxEpochs = 10\n",
    "loadSavedModel = False\n",
    "\n",
    "trackingLabels = ['person', 'car']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training/ Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = './data/images_thermal_train'\n",
    "valPath = './data/images_thermal_val'\n",
    "testPath = './data/video_thermal_test'\n",
    "dataDir = 'data'\n",
    "jsonFile = 'coco.json'\n",
    "\n",
    "jsonFiles = { \n",
    "              'train' : os.path.join(trainPath, jsonFile),\n",
    "              'val' : os.path.join(valPath, jsonFile),\n",
    "              'test' : os.path.join(testPath, jsonFile)\n",
    "            }\n",
    "\n",
    "imagePaths = { \n",
    "              'train' : trainPath,\n",
    "              'val' : valPath,\n",
    "              'test' : testPath\n",
    "            }\n",
    "\n",
    "for key, val in jsonFiles.items():\n",
    "    if os.path.isfile(val):\n",
    "        print(f'coco.json Exists: {key}, {val}')\n",
    "    \n",
    "for key, val in imagePaths.items():\n",
    "    if os.path.isdir(val):\n",
    "        print(f'Data Directory Exists: {key}, {val}')\n",
    "        \n",
    "labelMap = {\n",
    "            0:  'background',\n",
    "            1:  'person',\n",
    "            2:  'bike', #(renamed from \"bicycle\")\n",
    "            3:  'car', #(this includes pick-up trucks and vans)\n",
    "            4:  'motor', #(renamed from \"motorcycle\" for brevity)\n",
    "            6:  'bus',\n",
    "            7:  'train',\n",
    "            8:  'truck', #(semi/freight truck, excluding pickup truck)\n",
    "            10: 'light', #(renamed from \"traffic light\" for brevity)\n",
    "            11: 'hydrant', #(renamed \"fire hydrant\" for brevity)\n",
    "            12: 'sign', #(renamed from \"street sign\" for brevity)\n",
    "            17: 'dog',\n",
    "            37: 'skateboard',\n",
    "            73: 'stroller', #(four-wheeled carriage for a child, also called pram)\n",
    "            77: 'scooter',\n",
    "            79: 'other vehicle' #(less common vehicles like construction equipment and trailers)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalCocoDataset(Dataset):\n",
    "    def __init__(self, json_file, image_dir, labels, labelMap, transform=None):\n",
    "        self.json_file = json_file\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.labels = labels\n",
    "        self.labelMap = labelMap\n",
    "        self._load_json()\n",
    "\n",
    "    def _load_json(self):\n",
    "        with open(self.json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.annotations = data['annotations']\n",
    "        self.images = data['images']\n",
    "            \n",
    "    def _map_annotations_to_image(self, id, imageWidth, imageHeight):\n",
    "        \n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        for entry in self.annotations:\n",
    "            if entry['image_id'] == id:\n",
    "                if (self.labelMap[entry['category_id']] in self.labels):\n",
    "                    tmpBox = copy(entry['bbox'])\n",
    "                    tmpBox[0] = tmpBox[0] * (640 / imageWidth)\n",
    "                    tmpBox[1] = tmpBox[1] * (640 / imageHeight)\n",
    "                    tmpBox[2] = tmpBox[2] * (640 / imageWidth)\n",
    "                    tmpBox[3] = tmpBox[3] * (640 / imageHeight)\n",
    "                    bboxes.append(tmpBox)\n",
    "                    labels.append(entry['category_id'])\n",
    "        \n",
    "        if bboxes == []:\n",
    "            bboxes.append([0, 0, 640, 640])\n",
    "            labels.append(0)\n",
    "        \n",
    "        return bboxes, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.images[idx]\n",
    "        image_file = os.path.join(self.image_dir, f\"{image['file_name']}\")\n",
    "        img = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "        img = cv2.resize(img, (640, 640))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        bboxes, labels =  self._map_annotations_to_image(image['id'], image['width'], image['height']) \n",
    "        labels = torch.tensor(labels)\n",
    "        labels = labels.squeeze()  # Remove extra dimensions\n",
    "        bboxes = torch.tensor(bboxes, dtype=torch.float32)\n",
    "        \n",
    "        return img, labels, bboxes\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize image\n",
    "])\n",
    "\n",
    "trainData = ThermalCocoDataset(jsonFiles['train'], imagePaths['train'], trackingLabels, labelMap, transform=transform)\n",
    "valData = ThermalCocoDataset(jsonFiles['val'], imagePaths['val'], trackingLabels, labelMap, transform=transform)\n",
    "testData = ThermalCocoDataset(jsonFiles['test'], imagePaths['test'], trackingLabels, labelMap, transform=transform)\n",
    "\n",
    "trainLoader = DataLoader(trainData, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "valLoader = DataLoader(valData, batch_size=batchSize, shuffle=True, num_workers=numWorkers)\n",
    "testLoader = DataLoader(testData, batch_size=batchSize, shuffle=False, num_workers=numWorkers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corner_rect(bb, color='red'):\n",
    "    bb = np.array(bb, dtype=np.float32)\n",
    "    return plt.Rectangle((bb[0], bb[1]), bb[2], bb[3], color=color,\n",
    "                         fill=False, lw=1)\n",
    "\n",
    "def show_corner_bb(im, bb, c=None, cLabel='', color='red', createFig=False):\n",
    "    if createFig:\n",
    "        plt.figure(figsize=(6,6))\n",
    "        if not cLabel == '':\n",
    "            plt.title(f'{cLabel} Class: {c}')\n",
    "    plt.imshow(im.squeeze(), cmap=\"gray\")\n",
    "    plt.gca().add_patch(create_corner_rect(bb, color=color))\n",
    "    \n",
    "def plot_sample(image, labels, bboxes, labelMap):\n",
    "    \n",
    "    plt.imshow(image.squeeze(), cmap=\"gray\")  # Convert (C, H, W) tensor to (H, W, C) for plotting\n",
    "    try:\n",
    "        for bbox, label in zip(bboxes, labels):\n",
    "            x, y, w, h = bbox\n",
    "            plt.gca().add_patch(plt.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none'))\n",
    "            plt.text(x, y - 5, f'{labelMap[label]}', color='r')\n",
    "    except:\n",
    "        x, y, w, h = bboxes[0]\n",
    "        plt.gca().add_patch(plt.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none'))\n",
    "        plt.text(x, y - 5, f'{labelMap[labels]}', color='r')\n",
    "\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Imagery From Training Data\n",
    "\n",
    "### Original Images (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Training Data Sample')\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(trainData), size=(1,)).item()\n",
    "    image, labels, bboxes = trainData[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), labelMap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Images (Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Validation Data Sample')\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(valData), size=(1,)).item()\n",
    "    image, labels, bboxes = valData[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), labelMap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Images (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Testing Data Sample')\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(testData), size=(1,)).item()\n",
    "    image, labels, bboxes = testData[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), labelMap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the backbone network\n",
    "# class Backbone(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Backbone, self).__init__()\n",
    "#         # Convolutional layers for feature extraction\n",
    "#         self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Feature extraction\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "#         x = F.relu(self.conv4(x))\n",
    "#         return x\n",
    "\n",
    "# # Define the region proposal network\n",
    "# class RegionProposalNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(RegionProposalNetwork, self).__init__()\n",
    "#         # Anchor layer\n",
    "#         self.anchor_layer = nn.Conv2d(128, 9 * 4, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Generate anchor boxes\n",
    "#         anchors = self.anchor_layer(x)\n",
    "#         return anchors\n",
    "\n",
    "# # Define the object detector model\n",
    "# class ObjectDetector(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(ObjectDetector, self).__init__()\n",
    "#         self.backbone = Backbone()\n",
    "#         self.rpn = RegionProposalNetwork()\n",
    "#         # Classifier and bbox regressor\n",
    "#         self.cls_layer = nn.Linear(128 * 80 * 80, num_classes)  # Adjust the input size\n",
    "#         self.bbox_layer = nn.Linear(128 * 80 * 80, num_classes * 4)  # Adjust the input size\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Backbone feature extraction\n",
    "#         features = self.backbone(x)\n",
    "#         # Region Proposal Network\n",
    "#         anchors = self.rpn(features)\n",
    "#         # Flatten the features\n",
    "#         features_flat = features.view(features.size(0), -1)\n",
    "#         # Classifier and bbox regressor\n",
    "#         cls_scores = self.cls_layer(features_flat)\n",
    "#         bbox_deltas = self.bbox_layer(features_flat)\n",
    "#         return cls_scores, bbox_deltas, anchors\n",
    "\n",
    "# # Generate summary for ObjectDetector\n",
    "# summary(ObjectDetector(len(trackingLabels)), (1, 640, 640))\n",
    "# model = ObjectDetector(len(trackingLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDetection(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ClassificationDetection, self).__init__()\n",
    "        inplace = True\n",
    "        resnet = models.resnet50(weights=None)  # Fix typo here\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        layers = list(resnet.children())[:6]\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Linear(512, 64), \n",
    "                                         nn.ReLU(inplace=inplace), \n",
    "                                         nn.Linear(64, num_classes), \n",
    "                                         nn.Softmax(dim=1))\n",
    "        self.bb = nn.Sequential(nn.Linear(512, 64),\n",
    "                                nn.ReLU(inplace=inplace),\n",
    "                                nn.Linear(64, 4*num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        classifier = self.classifier(x)\n",
    "        print(classifier.shape)\n",
    "        bbox = self.bb(x)\n",
    "        return classifier, bbox\n",
    "    \n",
    "summary(ClassificationDetection(len(trackingLabels)), (1,640,640))\n",
    "\n",
    "model = ClassificationDetection(len(trackingLabels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Criterion and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=learnRate)\n",
    "print(f'Optimizer: {optimizer}')\n",
    "\n",
    "criterion_classifier = nn.CrossEntropyLoss()   \n",
    "print(f'\\nCriterion: {criterion_classifier}')\n",
    "\n",
    "criterion_bbox = nn.SmoothL1Loss()   \n",
    "print(f'\\nCriterion bbox: {criterion_bbox}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DetectionLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(DetectionLoss, self).__init__()\n",
    "#         self.cls_loss = nn.CrossEntropyLoss() ## Classifier\n",
    "#         self.reg_loss = nn.SmoothL1Loss() ## Bounding Box\n",
    "\n",
    "#     def forward(self, pred_cls, pred_reg, target_cls, target_reg):\n",
    "#         classification_loss = self.cls_loss(pred_cls, target_cls)\n",
    "#         regression_loss = self.reg_loss(pred_reg, target_reg)\n",
    "#         return classification_loss + regression_loss\n",
    "    \n",
    "# # Define optimizer and learning rate scheduler\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learnRate)\n",
    "# print(f'Optimizer: {optimizer}')\n",
    "\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Adjust the scheduler parameters\n",
    "# print(f'Scheduler: {scheduler}')\n",
    "\n",
    "# # Define the loss function\n",
    "# criterion = DetectionLoss()\n",
    "# print(f'\\nCriterion: {criterion}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)\n",
    "# criterion = criterion.to(device)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(maxEpochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for images, labels, bboxes in trainLoader:\n",
    "#         images, labels, bboxes = images.to(device), labels.to(device), bboxes.to(device)\n",
    "        \n",
    "#         # Zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         cls_scores, bbox_deltas, anchors = model(images)\n",
    "#         print(cls_scores.shape, cls_scores)\n",
    "#         print(labels.shape, labels)\n",
    "#         # Compute the loss\n",
    "#         loss = criterion(cls_scores, bbox_deltas, labels, bboxes)\n",
    "        \n",
    "#         # Backward pass and optimize\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print statistics\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     # Print epoch statistics\n",
    "#     print(f'Epoch [{epoch+1}/{maxEpochs}], Loss: {running_loss/len(trainLoader)}')\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_loss = 0.0\n",
    "#         for images, labels, bboxes in valLoader:\n",
    "#             images, labels, bboxes = images.to(device), labels.to(device), bboxes.to(device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             cls_scores, bbox_deltas, anchors = model(images)\n",
    "\n",
    "#             # Compute the loss\n",
    "#             loss = criterion(cls_scores, bbox_deltas, labels, bboxes)\n",
    "\n",
    "#             # Accumulate validation loss\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#         # Print validation statistics\n",
    "#         print(f'Validation Loss: {val_loss/len(valLoader)}')\n",
    "\n",
    "#     # Adjust learning rate\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion_classifier.to(device)\n",
    "criterion_bbox = criterion_bbox.to(device)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, maxEpochs+1):\n",
    "    epoch_loss = []\n",
    "    for images, labels, bboxes in trainLoader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        bboxes = bboxes.to(device)\n",
    "        print(images.shape, labels.shape, bboxes.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        classifier_output, bbox_output = model(images)\n",
    "        print(classifier_output.shape, labels.shape)\n",
    "        print(bbox_output.shape, bboxes.shape)\n",
    "        \n",
    "        # Compute loss\n",
    "        classifier_loss = criterion_classifier(classifier_output, labels)\n",
    "        bbox_loss = criterion_bbox(bbox_output, bboxes)\n",
    "        total_loss = classifier_loss + bbox_loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss.append(total_loss.item())\n",
    "        \n",
    "    losses.append(np.mean(epoch_loss))\n",
    "    print(f'[Epoch: {epoch}/{maxEpochs}] Loss: {np.round(losses[-1], 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model (If Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if loadModel:\n",
    "#     model = ClassificationDetection(len(labelMap), len(labelMap))\n",
    "#     optimizer = optimizer\n",
    "#     checkpoint = torch.load(modelPath)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     epoch = checkpoint['epoch']\n",
    "#     criterion = checkpoint['loss']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)\n",
    "# criterion = criterion.to(device)\n",
    "# criterion_bbox = criterion_bbox.to(device)\n",
    "\n",
    "# if not loadModel:\n",
    "     \n",
    "#     exp_lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
    "#     trainLoss = []\n",
    "\n",
    "#     for epoch in range(1, maxEpochs+1):\n",
    "#         epochLoss = []\n",
    "#         testEpochLoss = []\n",
    "#         model.train()\n",
    "\n",
    "#         for i, (images, bbox, labels) in enumerate(trainLoader):\n",
    "#             optimizer.zero_grad()\n",
    "#             images = images.to(device)\n",
    "#             bbox = bbox.to(device)\n",
    "#             labels = labels.to(device)         \n",
    "#             predLabels, predBoxes = model(images)\n",
    "#             lossLabels = criterion(predLabels, labels.to(device))\n",
    "#             lossBoxes = criterion_bbox(predBoxes, bbox).sum(1)\n",
    "#             loss = lossLabels + lossBoxes\n",
    "#             loss.backward()          \n",
    "#             optimizer.step()\n",
    "#             lossVal = loss.item()\n",
    "#             epochLoss.append(float(lossVal))\n",
    "\n",
    "#         trainLoss.append(np.mean(epochLoss))\n",
    "#         exp_lr_scheduler.step(trainLoss[-1])\n",
    "                \n",
    "#         print(f'[Epoch: {epoch}/{maxEpochs}] Loss: {np.round(trainLoss[-1], 5)}')\n",
    "\n",
    "#         if saveModel and epoch % 5 == 0: ## save model every 5th epoch\n",
    "#             torch.save({\n",
    "#                     'epoch': epoch,\n",
    "#                     'model_state_dict': model.state_dict(),\n",
    "#                     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                     'loss': loss,\n",
    "#                     }, modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model (If Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if saveModel and not loadModel:\n",
    "#     torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': criterion,\n",
    "#             'loss_bbox': criterion_bbox,\n",
    "#             }, modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not loadModel:\n",
    "#     print(f'Final MSE ({maxEpochs} epochs): {losses[-1]}\\n')\n",
    "    \n",
    "#     f = plt.figure(figsize=(10,8))\n",
    "#     plt.plot(losses, label=\"train\")\n",
    "#     plt.xlabel(\"epochs\")\n",
    "#     plt.ylabel(\"cross entropy\")\n",
    "#     plt.title(\"Epochs vs. Loss Function\")\n",
    "#     plt.legend()\n",
    "#     plt.grid()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Imagery of Model Output\n",
    "\n",
    "### Training Images Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainImages, trainLabels, trainImagesFlipped = next(iter(trainLoader))\n",
    "# samples = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs, __ = model(trainImages.to(device))\n",
    "\n",
    "# j = 0\n",
    "# for i, label in enumerate(trainLabels):\n",
    "#     if label.item() in samples:\n",
    "#         f = plt.figure(figsize=(12, 4))      \n",
    "#         ax1 = f.add_subplot(131)\n",
    "#         ax1.imshow(trainImages[i].squeeze(), cmap='gray')\n",
    "#         ax1.axis('off')\n",
    "#         ax1.set_title(f'Original - {label.item()}')\n",
    "#         ax2 = f.add_subplot(132)\n",
    "#         ax2.imshow(outputs[i].detach().cpu()[0].squeeze(), cmap='gray')\n",
    "#         ax2.axis('off')\n",
    "#         ax2.set_title(f'Reconstruction (Flipped) - {label.item()}')\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "        \n",
    "#         samples.remove(label.item())\n",
    "#         j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Images Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testImages, testLabels, testImagesFlipped = next(iter(testLoader))\n",
    "# samples = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs, __ = model(testImages.to(device))\n",
    "\n",
    "# j = 0\n",
    "# for i, label in enumerate(testLabels):\n",
    "#     if label.item() in samples:\n",
    "#         f = plt.figure(figsize=(12, 4))      \n",
    "#         ax1 = f.add_subplot(131)\n",
    "#         ax1.imshow(testImages[i].squeeze(), cmap='gray')\n",
    "#         ax1.axis('off')\n",
    "#         ax1.set_title(f'Original - {label.item()}')\n",
    "#         ax2 = f.add_subplot(132)\n",
    "#         ax2.imshow(outputs[i].detach().cpu()[0].squeeze(), cmap='gray')\n",
    "#         ax2.axis('off')\n",
    "#         ax2.set_title(f'Reconstruction (Flipped) - {label.item()}')\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "        \n",
    "#         samples.remove(label.item())\n",
    "#         j += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
