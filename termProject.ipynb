{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Project\n",
    "\n",
    "Team: Rain Price, Weston Scott\n",
    "\n",
    "ECE 523 | Engineering Applications of Machine Learning and Data Analytics\n",
    "\n",
    "Professor Abhijit Mahalanobis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle and People Detection with the FLIR Thermal Dataset\n",
    "\n",
    "![alt text](problemStatement.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Get simple training model working\n",
    "- Get resulting images and predictions showing\n",
    "- Run all images through model and check error\n",
    "- Rewrite Resnet layers to be our own homegrown solution?\n",
    "- Survive this class ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from PIL import Image, ImageFilter\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xml.etree.ElementTree as ET\n",
    "from torchvision.transforms.functional import pad\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import ToTensor, transforms\n",
    "from copy import copy\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Seed, Device Architecture, and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "randomSeed = 2024\n",
    "np.random.seed(randomSeed)\n",
    "torch.manual_seed(randomSeed)\n",
    "\n",
    "print(f'PyTorch Version: {torch.__version__}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print(f'PyTorch Device: {device}')\n",
    "\n",
    "numWorkers = 2\n",
    "normalizeImages = False\n",
    "trackingLabels = ['person', 'car']\n",
    "colors = ['m', 'b']\n",
    "maxObjects = 83\n",
    "numClasses = len(trackingLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxEpochsClassifier = 10\n",
    "batchSizeClass = 64\n",
    "learnRateClass = 0.001\n",
    "classifierModelPath = './classificationModel_20240502.pt'\n",
    "\n",
    "saveClassifierModel = True\n",
    "loadClassifierModel = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSizeDetect = 32\n",
    "maxEpochsDetect = 10\n",
    "learnRateDetect = 0.001\n",
    "detectorModelPath = './DetectionModel_20240502.pt'\n",
    "\n",
    "saveDetectionModel = True\n",
    "loadDetectionModel = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to see if input images are the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/images_thermal_train/data'\n",
    "\n",
    "imgs = os.listdir(path)\n",
    "sizes = []\n",
    "for img in imgs:\n",
    "    filename = os.path.join(path, img)\n",
    "    image = Image.open(filename)\n",
    "    sizes.append(image.size)\n",
    "print(sizes[-1])\n",
    "np.unique(sizes, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training/ Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = './data/images_thermal_train'\n",
    "valPath = './data/images_thermal_val'\n",
    "testPath = './data/video_thermal_test'\n",
    "dataDir = 'data'\n",
    "jsonFile = 'coco.json'\n",
    "trimmedJsonFile = 'trimmed_coco.json'\n",
    "\n",
    "jsonFiles = { \n",
    "              'train' : os.path.join(trainPath, jsonFile),\n",
    "              'val' : os.path.join(valPath, jsonFile),\n",
    "              'test' : os.path.join(testPath, jsonFile)\n",
    "            }\n",
    "\n",
    "jsonFilesTrimmed = { \n",
    "              'train' : os.path.join(trainPath, trimmedJsonFile),\n",
    "              'val' : os.path.join(valPath, trimmedJsonFile),\n",
    "              'test' : os.path.join(testPath, trimmedJsonFile)\n",
    "            }\n",
    "\n",
    "imagePaths = { \n",
    "              'train' : trainPath,\n",
    "              'val' : valPath,\n",
    "              'test' : testPath\n",
    "            }\n",
    "\n",
    "for key, val in jsonFiles.items():\n",
    "    if os.path.isfile(val):\n",
    "        print(f'coco.json Exists: {key}, {val}')\n",
    "    \n",
    "for key, val in imagePaths.items():\n",
    "    if os.path.isdir(val):\n",
    "        print(f'Data Directory Exists: {key}, {val}')\n",
    "        \n",
    "labelMap = {\n",
    "            1:  'person',\n",
    "            2:  'bike', #(renamed from \"bicycle\")\n",
    "            3:  'car', #(this includes pick-up trucks and vans)\n",
    "            4:  'motor', #(renamed from \"motorcycle\" for brevity)\n",
    "            6:  'bus',\n",
    "            7:  'train',\n",
    "            8:  'truck', #(semi/freight truck, excluding pickup truck)\n",
    "            10: 'light', #(renamed from \"traffic light\" for brevity)\n",
    "            11: 'hydrant', #(renamed \"fire hydrant\" for brevity)\n",
    "            12: 'sign', #(renamed from \"street sign\" for brevity)\n",
    "            17: 'dog',\n",
    "            18: 'deer',\n",
    "            37: 'skateboard',\n",
    "            73: 'stroller', #(four-wheeled carriage for a child, also called pram)\n",
    "            75: 'scooter',\n",
    "            79: 'other vehicle' #(less common vehicles like construction equipment and trailers)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxMaxWidth = 1\n",
    "boxMaxHeight = 1\n",
    "for key, val in jsonFiles.items():\n",
    "    \n",
    "    print(f\"Generating trimmed coco.json files from: {val} for {key} data at: {jsonFilesTrimmed[key]} ...\")\n",
    "    with open(val, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "    imageIDs = []\n",
    "    for annot in data['annotations']:\n",
    "        if labelMap[annot['category_id']] in trackingLabels:\n",
    "            annotations.append(annot)\n",
    "            imageIDs.append(annot['image_id'])\n",
    "            \n",
    "            bbox = annot['bbox']\n",
    "            if bbox[2] > boxMaxWidth:\n",
    "                boxMaxWidth = bbox[2]\n",
    "            if bbox[3] > boxMaxHeight:\n",
    "                boxMaxHeight = bbox[3]\n",
    "                \n",
    "    imageIDs= np.unique(imageIDs)\n",
    "    \n",
    "    for i, image in enumerate(data['images']):\n",
    "        if (image['id'] in imageIDs):\n",
    "            images.append(image)\n",
    "    \n",
    "    trimmedData = { 'annotations': annotations,\n",
    "                    'images': images,\n",
    "                    'ids': dict(enumerate(trackingLabels))}\n",
    "    \n",
    "    with open(jsonFilesTrimmed[key], \"w\") as outfile: \n",
    "        json.dump(trimmedData, outfile, indent=4)\n",
    "        \n",
    "##TODO: Calculate mean and std of images, calculate maxObjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxMaxWidth =  int(boxMaxWidth * 640/image['width'])\n",
    "boxMaxHeight = int(boxMaxHeight * 640/image['height'])\n",
    "\n",
    "print(f'Maximum Object Box Width: {boxMaxWidth}')\n",
    "print(f'Maximum Object Box Height: {boxMaxHeight}')\n",
    "print(f'Maximum Number of Objects: {maxObjects}')\n",
    "print(f'Number of Classes: {numClasses}')\n",
    "\n",
    "del data, images, annotations, imageIDs, trimmedData, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizeBoxImage(img, bbox):\n",
    "    x, y, w, h = bbox\n",
    "    x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "    img = img[y:y+h, x:x+w]\n",
    "    if w <= 80 and h <= 80:\n",
    "        img = cv2.resize(img, (80, 80))\n",
    "    elif w <= 160 and h <= 160:\n",
    "        img = cv2.resize(img, (160, 160))\n",
    "        img = cv2.pyrDown(img)\n",
    "    if w <= 320 and h <= 320:\n",
    "        img = cv2.resize(img, (320, 320))\n",
    "        img = cv2.pyrDown(img)\n",
    "        img = cv2.pyrDown(img)\n",
    "    else:\n",
    "        img = cv2.resize(img, (640, 640))\n",
    "        img = cv2.pyrDown(img)\n",
    "        img = cv2.pyrDown(img)\n",
    "        img = cv2.pyrDown(img) ## final image size is 80x80\n",
    "    return img\n",
    "\n",
    "class ThermalCocoDataset(Dataset):\n",
    "    def __init__(self, jsonFile:str, imageDir:str, trackingLabels:list, labelMap:dict, maxWidth:int=2, maxHeight:int=2, singleObject:bool=False, transform=None, maxObjects:int=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            jsonFile (str): Path to the COCO-style JSON file containing annotations.\n",
    "            imageDir (str): Directory containing the images.\n",
    "            trackingLabels (list): List of tracking labels.\n",
    "            labelMap (dict): Mapping of category IDs to labels.\n",
    "            single_object (bool): Whether to treat each image as containing a single object.\n",
    "            transform (callable, optional): Optional transform to be applied to the images.\n",
    "            maxObjects (int): The max number of objects to process in an image.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.jsonFile = jsonFile\n",
    "        self.imageDir = imageDir\n",
    "        self.transform = transform\n",
    "        self.trackingLabels = trackingLabels\n",
    "        self.labelMap = labelMap\n",
    "        self.maxWidth = maxWidth\n",
    "        self.maxHeight = maxHeight\n",
    "        self.singleObject = singleObject\n",
    "        self.maxObjects = maxObjects\n",
    "        self._load_json()\n",
    "\n",
    "    def _load_json(self):\n",
    "        \"\"\"Load JSON annotations.\"\"\"\n",
    "        \n",
    "        with open(self.jsonFile, 'r') as f:\n",
    "            data = json.load(f)                    \n",
    "        \n",
    "        self.annotations = data['annotations']\n",
    "        self.images = data['images']\n",
    "            \n",
    "    def _adjust_bounding_box(self, bbox, width, height):\n",
    "        \"\"\"Adjust bounding box coordinates to match resized image.\"\"\"\n",
    "        \n",
    "        if self.singleObject:\n",
    "            return [int(bbox[0] * (640 / width)),\n",
    "                    int(bbox[1] * (640 / height)),\n",
    "                    int(bbox[2] * (640 / width)),\n",
    "                    int(bbox[3] * (640 / height))]\n",
    "        else:\n",
    "            return [int(bbox[0] * (320 / width)),\n",
    "                    int(bbox[1] * (320 / height)),\n",
    "                    int(bbox[2] * (320 / width)),\n",
    "                    int(bbox[3] * (320 / height))]\n",
    "    \n",
    "    def _get_single_object(self, idx):\n",
    "        \"\"\"Get image, label, bounding box, and number of objects for single object mode.\"\"\"\n",
    "        \n",
    "        annotation = self.annotations[idx]\n",
    "        image_id = annotation['image_id']\n",
    "        for entry in self.images:\n",
    "            if entry['id'] == image_id:\n",
    "                image_file_name = entry['file_name']\n",
    "                width = entry['width']\n",
    "                height = entry['height']\n",
    "            \n",
    "        image_file = os.path.join(self.imageDir, f\"{image_file_name}\")\n",
    "        img = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "        img = cv2.resize(img, (640, 640))\n",
    "\n",
    "        if self.labelMap[annotation['category_id']] in self.trackingLabels:\n",
    "            # Assuming annotation format: [x, y, width, height]\n",
    "            bbox = self._adjust_bounding_box(annotation['bbox'], width, height)\n",
    "            tmpLabel = self.labelMap[annotation['category_id']]\n",
    "            \n",
    "        img = sizeBoxImage(img, bbox)\n",
    "        label = torch.tensor(self.trackingLabels.index(tmpLabel))\n",
    "        bbox = torch.tensor(bbox)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label, bbox, torch.tensor(1)\n",
    "\n",
    "    def _get_multi_objects(self, idx):\n",
    "        \"\"\"Get image, labels, bounding boxes, and number of objects for multi-object mode.\"\"\"\n",
    "        \n",
    "        image = self.images[idx]\n",
    "        image_file = os.path.join(self.imageDir, f\"{image['file_name']}\")\n",
    "        img = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "        img = cv2.resize(img, (640, 640))\n",
    "            \n",
    "        id = image['id']\n",
    "        width = image['width']\n",
    "        height = image['height']\n",
    "        \n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        for entry in self.annotations:\n",
    "            if entry['image_id'] == id:\n",
    "                if (self.labelMap[entry['category_id']] in self.trackingLabels):\n",
    "                    tmpLabel = self.labelMap[entry['category_id']]\n",
    "                    bboxes.append(self._adjust_bounding_box(entry['bbox'], width, height))\n",
    "                    labels.append(self.trackingLabels.index(tmpLabel))\n",
    "                    \n",
    "        numObjects = torch.tensor(len(labels))\n",
    "        padded_labels = torch.full((self.maxObjects,), 0)\n",
    "        padded_labels[:len(labels)] = torch.tensor(labels)[:self.maxObjects].squeeze()\n",
    "\n",
    "        padded_bboxes = torch.full((self.maxObjects,4), 0, dtype=torch.float32)\n",
    "        padded_bboxes[:len(bboxes)] = torch.tensor(bboxes, dtype=torch.float32)[:self.maxObjects]\n",
    "\n",
    "        if self.transform:\n",
    "            img = cv2.pyrDown(img)\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, padded_labels, padded_bboxes, numObjects\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of images in the dataset.\"\"\"\n",
    "        \n",
    "        if self.singleObject:\n",
    "            return len(self.annotations)\n",
    "        else:\n",
    "            return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get the image, labels, and bounding boxes for the given index.\"\"\"\n",
    "        \n",
    "        if self.singleObject:\n",
    "            return self._get_single_object(idx)\n",
    "        else:\n",
    "            return self._get_multi_objects(idx)\n",
    "\n",
    "if normalizeImages:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
    "        transforms.Normalize(mean=[262.6299], std=[117.4840])  # Normalize image\n",
    "    ])\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataClass = ThermalCocoDataset(jsonFilesTrimmed['train'], imagePaths['train'], trackingLabels, labelMap, 640, 640, True, transform=transform)\n",
    "valDataClass = ThermalCocoDataset(jsonFilesTrimmed['val'], imagePaths['val'], trackingLabels, labelMap, 640, 640, True, transform=transform)\n",
    "testDataClass = ThermalCocoDataset(jsonFilesTrimmed['test'], imagePaths['test'], trackingLabels, labelMap, 640, 640, True, transform=transform)\n",
    "\n",
    "trainLoaderClass = DataLoader(trainDataClass, batch_size=batchSizeClass, shuffle=True, num_workers=numWorkers)\n",
    "valLoaderClass = DataLoader(valDataClass, batch_size=batchSizeClass, shuffle=True, num_workers=numWorkers)\n",
    "testLoaderClass = DataLoader(testDataClass, batch_size=batchSizeClass, shuffle=False, num_workers=numWorkers)\n",
    "\n",
    "trainDataDetect = ThermalCocoDataset(jsonFilesTrimmed['train'], imagePaths['train'], trackingLabels, labelMap, transform=transform, maxObjects=maxObjects)\n",
    "valDataDetect = ThermalCocoDataset(jsonFilesTrimmed['val'], imagePaths['val'], trackingLabels, labelMap, transform=transform, maxObjects=maxObjects)\n",
    "testDataDetect = ThermalCocoDataset(jsonFilesTrimmed['test'], imagePaths['test'], trackingLabels, labelMap, transform=transform, maxObjects=maxObjects)\n",
    "\n",
    "trainLoaderDetect = DataLoader(trainDataDetect, batch_size=batchSizeDetect, shuffle=False, num_workers=numWorkers)\n",
    "valLoaderDetect = DataLoader(valDataDetect, batch_size=batchSizeDetect, shuffle=False, num_workers=numWorkers)\n",
    "testLoaderDetect = DataLoader(testDataDetect, batch_size=1, shuffle=False, num_workers=numWorkers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corner_rect(bb, color='red'):\n",
    "    bb = np.array(bb, dtype=np.float32)\n",
    "    return plt.Rectangle((bb[0], bb[1]), bb[2], bb[3], color=color,\n",
    "                         fill=False, lw=1)\n",
    "\n",
    "def show_corner_bb(im, bb, c=None, cLabel='', color='red', createFig=False):\n",
    "    if createFig:\n",
    "        plt.figure(figsize=(6,6))\n",
    "        if not cLabel == '':\n",
    "            plt.title(f'{cLabel} Class: {c}')\n",
    "    plt.imshow(im.squeeze(), cmap=plt.cm.gray)\n",
    "    plt.gca().add_patch(create_corner_rect(bb, color=color))\n",
    "    \n",
    "def plot_sample(image, labels, bboxes, num, showbb=True):\n",
    "    \n",
    "    plt.imshow(image.squeeze(), cmap=\"gray\")  # Convert (C, H, W) tensor to (H, W, C) for plotting\n",
    "    if showbb:\n",
    "        plt.title(f'Number of Objects: {num}')\n",
    "        try:\n",
    "            for bbox, label in zip(bboxes, labels):\n",
    "                label = int(label)\n",
    "                x, y, w, h = bbox\n",
    "                plt.gca().add_patch(plt.Rectangle((x, y), w, h, linewidth=1, edgecolor=colors[label], facecolor='none'))\n",
    "                plt.text(x, y-5, f'{trackingLabels[label]}', color=colors[label])\n",
    "        except:\n",
    "            try:\n",
    "                x, y, w, h = bboxes\n",
    "            except:\n",
    "                x, y, w, h = bboxes[0]\n",
    "            plt.gca().add_patch(plt.Rectangle((x, y), w, h, linewidth=1, edgecolor=colors[labels], facecolor='none'))\n",
    "            plt.text(x, y-5, f'{trackingLabels[labels]}', color=colors[labels])\n",
    "        plt.axis('off')\n",
    "    else:\n",
    "        plt.title(f'Sample {trackingLabels[labels]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Imagery From Training Data\n",
    "\n",
    "### Original Images (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Training Data Sample')\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(trainDataClass), size=(1,)).item()\n",
    "    image, labels, bboxes, num = trainDataClass[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), num.item(), showbb=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Training Data Sample')\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(trainDataDetect), size=(1,)).item()\n",
    "    image, labels, bboxes, num = trainDataDetect[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    labels = labels[:num]\n",
    "    bboxes = bboxes[:num,:]\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), num.item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Images (Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Training Data Sample')\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(valDataClass), size=(1,)).item()\n",
    "    image, labels, bboxes, num = valDataClass[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), num.item(), showbb=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Validation Data Sample')\n",
    "\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(valDataDetect), size=(1,)).item()\n",
    "    image, labels, bboxes, num = valDataDetect[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    labels = labels[:num]\n",
    "    bboxes = bboxes[:num,:]\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), num.item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Images (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Training Data Sample')\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(testDataClass), size=(1,)).item()\n",
    "    image, labels, bboxes, num = testDataClass[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), num.item(), showbb=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Testing Data Sample')\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(testDataDetect), size=(1,)).item()\n",
    "    image, labels, bboxes, num = testDataDetect[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    labels = labels[:num]\n",
    "    bboxes = bboxes[:num,:]\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), num.item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        inplace = False\n",
    "        self.batchNorm = nn.BatchNorm2d(1)\n",
    "        resnet = models.resnet18(weights=None)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        layers = list(resnet.children())[:6]\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Linear(128, 32), \n",
    "                                         nn.ReLU(inplace=inplace), \n",
    "                                         nn.Linear(32, num_classes),\n",
    "                                         nn.Softmax(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batchNorm(x)\n",
    "        x = self.features(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        classifier_output = self.classifier(x)\n",
    "        return classifier_output\n",
    "    \n",
    "classifier = Classifier(numClasses)\n",
    "\n",
    "classifier = classifier.cpu()\n",
    "\n",
    "if str(device) == 'cuda':\n",
    "    classifier = classifier.to(device)\n",
    "\n",
    "summary(classifier, (1,80,80))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterionClassifier = nn.CrossEntropyLoss()   \n",
    "print(f'Criterion: {criterionClassifier}')\n",
    "\n",
    "optimizerClassifier = optim.Adam(classifier.parameters(), lr = learnRateClass)   \n",
    "print(f'\\nOptimizer: {optimizerClassifier}')\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizerClassifier, mode='min')\n",
    "# print(f'\\nscheduler: {scheduler}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loadClassifierModel:\n",
    "    classifier = Classifier(numClasses)\n",
    "    checkpoint = torch.load(classifierModelPath)\n",
    "    classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizerClassifier.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    trainLoss = [checkpoint['loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.to(device)\n",
    "criterionClassifier = criterionClassifier.to(device)\n",
    "\n",
    "if not loadClassifierModel:\n",
    "        \n",
    "    trainLoss = []\n",
    "    trainAcc = []\n",
    "    testLoss = []\n",
    "    testAcc = []\n",
    "\n",
    "    for epoch in range(1, maxEpochsClassifier+1):\n",
    "        epochLoss = []\n",
    "        epochAcc = []\n",
    "        testEpochLoss = []\n",
    "        testEpochAcc = []\n",
    "        classifier.train()\n",
    "\n",
    "        for i, (images, labels, __, __) in tqdm(enumerate(trainLoaderClass)):\n",
    "            optimizerClassifier.zero_grad()\n",
    "            \n",
    "            if str(device) == 'cuda':\n",
    "                images = images.to(device)\n",
    "                outputs = classifier(images)\n",
    "                labels = labels.to(device)\n",
    "            \n",
    "            loss = criterionClassifier(outputs, labels)\n",
    "            loss.backward()          \n",
    "            optimizerClassifier.step()\n",
    "            lossVal = loss.item()\n",
    "            \n",
    "            pred = torch.max(outputs, 1)[1].data.squeeze()\n",
    "            accuracy = (pred == labels).sum().item() / float(labels.size(0))\n",
    "\n",
    "            epochLoss.append(float(lossVal))\n",
    "            epochAcc.append(float(accuracy))\n",
    "                \n",
    "        trainLoss.append(np.mean(epochLoss))\n",
    "        trainAcc.append(np.mean(epochAcc))\n",
    "        # scheduler.step(trainLoss[-1])\n",
    "                \n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            for testImages, testLabels, __, __ in tqdm(valLoaderClass):\n",
    "                testImages = testImages.to(device)\n",
    "                testLabels = testLabels.to(device)\n",
    "                    \n",
    "                testOutput = classifier(testImages)\n",
    "                lossVal = criterionClassifier(testOutput, testLabels)\n",
    "            \n",
    "                if str(device) == 'cuda':\n",
    "                    lossVal = lossVal.cpu()\n",
    "                    \n",
    "                predVal = torch.max(testOutput, 1)[1].data.squeeze()\n",
    "                accuracyVal = (predVal == testLabels).sum().item() / float(testLabels.size(0))\n",
    "                \n",
    "                testEpochLoss.append(lossVal)\n",
    "                testEpochAcc.append(accuracyVal)\n",
    "                \n",
    "            testLoss.append(np.mean(testEpochLoss))\n",
    "            testAcc.append(np.mean(testEpochAcc))\n",
    "                \n",
    "        print(f'[Epoch: {epoch}/{maxEpochsClassifier}] Loss: {np.round(trainLoss[-1], 5)}')\n",
    "\n",
    "        if saveClassifierModel and epoch % 1 == 0: ## save model every 5th epoch\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': classifier.state_dict(),\n",
    "                    'optimizer_state_dict': optimizerClassifier.state_dict(),\n",
    "                    'loss': trainLoss[-1],\n",
    "                    }, classifierModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if saveClassifierModel:\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': classifier.state_dict(),\n",
    "            'optimizer_state_dict': optimizerClassifier.state_dict(),\n",
    "            'loss': trainLoss[-1],\n",
    "            }, classifierModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not loadClassifierModel:\n",
    "    print(f'Final MSE ({maxEpochsClassifier} epochs): {trainLoss[-1]}\\n')\n",
    "    \n",
    "    f = plt.figure(figsize=(10,8))\n",
    "    plt.plot(trainLoss, label=\"train\")\n",
    "    plt.plot(testLoss, label=\"val\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"CrossEntropy\")\n",
    "    plt.title(\"Epochs vs. Loss Function\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    f = plt.figure(figsize=(10,8))\n",
    "    plt.plot(trainAcc, label=\"train\")\n",
    "    plt.plot(testAcc, label=\"test\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(\"Epochs vs. Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "actuals = []\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels, __, __ in tqdm(testLoaderClass):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "            \n",
    "        test_output = classifier(images)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "        accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "        predictions.extend(pred_y.cpu().numpy())\n",
    "        actuals.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorCount = [predictions[i] == actuals[i] for i in range(len(actuals))]\n",
    "print(f\"\\nCorrect classifications on {len(predictions)} images: {np.sum(errorCount)}/{len(predictions)} | {np.sum(errorCount)/len(predictions)*100}%\")\n",
    "\n",
    "## Create Confusion matrix\n",
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "confusion_matrix = metrics.confusion_matrix(actuals, predictions)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels=trackingLabels)\n",
    "\n",
    "cm_display.plot(ax=ax)\n",
    "plt.title(\"Confusion Matrix for Test Data\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions).reshape(len(predictions), 1)\n",
    "actuals = np.array(actuals).reshape(len(actuals), 1)\n",
    "bidxs = np.where(actuals != predictions)[0]\n",
    "gidxs = np.where(actuals == predictions)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle(f'Correct Classifications: {np.round(len(gidxs)/len(predictions),4)*100}% (Actual - Predicted)')\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(gidxs), size=(1,)).item()\n",
    "    sample_idx = gidxs[sample_idx]\n",
    "    img, label, __, __ = testDataClass[sample_idx]\n",
    "    pred = predictions[sample_idx]\n",
    "    act = actuals[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(f'{trackingLabels[label]} - {trackingLabels[pred[0]]}')\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle(f'Incorrect Classifications: {np.round(len(bidxs)/len(predictions),4)*100}% (Actual - Predicted)')\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(bidxs), size=(1,)).item()\n",
    "    sample_idx = bidxs[sample_idx]\n",
    "    img, label, __, __ = testDataClass[sample_idx]\n",
    "    pred = predictions[sample_idx]\n",
    "    act = actuals[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(f'{trackingLabels[label]} - {trackingLabels[pred[0]]}')\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(nn.Module):\n",
    "    def __init__(self, max_objects):\n",
    "        super(Detector, self).__init__()\n",
    "        inplace = False\n",
    "        \n",
    "        self.batchNorm = nn.BatchNorm2d(1)\n",
    "        resnet = models.resnet18(weights=None)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        layers = list(resnet.children())[:6]\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        self.max_objects = max_objects\n",
    "        \n",
    "        self.bb = nn.Sequential(nn.Linear(128, 64),\n",
    "                                nn.ReLU(inplace=inplace),\n",
    "                                nn.Linear(64, 4*max_objects))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batchNorm(x)\n",
    "        x = self.features(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        bbox_output = self.bb(x)\n",
    "        return bbox_output\n",
    "    \n",
    "detector = Detector(maxObjects)\n",
    "\n",
    "detector = detector.cpu()\n",
    "\n",
    "if str(device) == 'cuda':\n",
    "    detector = detector.to(device)\n",
    "\n",
    "summary(detector, (1,320,320))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Criterion and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DetectionLoss, self).__init__()\n",
    "        self.bboxLoss = nn.SmoothL1Loss()\n",
    "\n",
    "    def forward(self, bboxes, num, bboxesPred):\n",
    "        maxNum = torch.max(num)       \n",
    "        preds = bboxesPred[:, :maxNum, :4]\n",
    "        truths = bboxes[:, :maxNum, :4]\n",
    "        preds = torch.flatten(preds)\n",
    "        truths = torch.flatten(truths)\n",
    "        loss = self.bboxLoss(preds, truths)\n",
    "        return loss\n",
    "    \n",
    "# Define the loss function\n",
    "criterionDetector = DetectionLoss()\n",
    "print(f'\\nCriterion: {criterionDetector}')\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizerDetector = optim.Adam(detector.parameters(), lr=learnRateDetect)\n",
    "print(f'Optimizer: {optimizerDetector}')\n",
    "\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Adjust the scheduler parameters\n",
    "# print(f'Scheduler: {scheduler}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model (If Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loadDetectionModel:\n",
    "    detector = Detector(maxObjects)\n",
    "    checkpoint = torch.load(detectorModelPath)\n",
    "    detector.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizerDetector.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()          \n\u001b[1;32m     28\u001b[0m     optimizerDetector\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 29\u001b[0m     lossVal \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     30\u001b[0m     epochLoss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(lossVal))\n\u001b[1;32m     31\u001b[0m trainLossDetector\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(epochLoss))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "detector = detector.to(device)\n",
    "criterionDetector = criterionDetector.to(device)\n",
    "\n",
    "if not loadDetectionModel:\n",
    "        \n",
    "    trainLossDetector = []\n",
    "    testLossDetector = []\n",
    "\n",
    "    for epoch in range(1, maxEpochsDetect+1):\n",
    "        epochLoss = []\n",
    "        epochAcc = []\n",
    "        testEpochLoss = []\n",
    "        testEpochAcc = []\n",
    "        detector.train()\n",
    "\n",
    "        for i, (images, __, bboxes, num) in tqdm(enumerate(trainLoaderDetect)):\n",
    "            optimizerDetector.zero_grad()\n",
    "            \n",
    "            if str(device) == 'cuda':\n",
    "                images = images.to(device)\n",
    "                bboxes = bboxes.to(device)\n",
    "                num = num.to(device)\n",
    "                \n",
    "            outputs = detector(images)\n",
    "            outputs = outputs.view(-1, maxObjects, 4)\n",
    "            loss = criterionDetector(bboxes, num, outputs)\n",
    "            loss.backward()          \n",
    "            optimizerDetector.step()\n",
    "            lossVal = loss.item()\n",
    "            epochLoss.append(float(lossVal))\n",
    "        trainLossDetector.append(np.mean(epochLoss))\n",
    "                \n",
    "        detector.eval()\n",
    "        with torch.no_grad():\n",
    "            for testImages, __, testBoxes, num, in tqdm(valLoaderDetect):\n",
    "                if str(device) == 'cuda':\n",
    "                    testImages = testImages.to(device)\n",
    "                    testBoxes = testBoxes.to(device)\n",
    "                    num = num.to(device)\n",
    "                    \n",
    "                testOutputs = detector(testImages)\n",
    "                testOutputs = testOutputs.view(-1, maxObjects, 4)\n",
    "                lossVal = criterionDetector(testBoxes, num, testOutputs)\n",
    "            \n",
    "                if str(device) == 'cuda':\n",
    "                    lossVal = lossVal.cpu()  \n",
    "                testEpochLoss.append(lossVal) \n",
    "            testLossDetector.append(np.mean(testEpochLoss))\n",
    "                \n",
    "        print(f'[Epoch: {epoch}/{maxEpochsDetect}] Loss: {np.round(trainLossDetector[-1], 5)}')\n",
    "\n",
    "        if saveDetectionModel and epoch % 1 == 0: ## save model every 5th epoch\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': detector.state_dict(),\n",
    "                    'optimizer_state_dict': optimizerDetector.state_dict(),\n",
    "                    'loss': trainLossDetector[-1],\n",
    "                    }, detectorModelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model (If Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if saveDetectionModel:\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': detector.state_dict(),\n",
    "            'optimizer_state_dict': optimizerDetector.state_dict(),\n",
    "            'loss': trainLossDetector[-1],\n",
    "            }, detectorModelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not loadDetectionModel:\n",
    "    print(f'Final MSE ({maxEpochsDetect} epochs): {trainLossDetector[-1]}\\n')\n",
    "    \n",
    "    f = plt.figure(figsize=(10,8))\n",
    "    plt.plot(trainLossDetector, label=\"train\")\n",
    "    plt.plot(testLossDetector, label=\"val\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"SmoothL1Loss\")\n",
    "    plt.title(\"Epochs vs. Loss Function\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "actuals = []\n",
    "detector.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels, bboxes, num in tqdm(testLoaderDetect):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        bboxes = bboxes.to(device)\n",
    "        num = num.to(device)\n",
    "        \n",
    "        predBoxes = detector(images)\n",
    "        predBoxes = predBoxes.view(-1, maxObjects, 4)\n",
    "        \n",
    "        predLabels = []\n",
    "        predScores = []\n",
    "        boxesKeep = []\n",
    "        for box in predBoxes.squeeze():\n",
    "            if not torch.all(box > 1):\n",
    "                continue\n",
    "\n",
    "            img = sizeBoxImage(images.squeeze().detach().cpu().numpy(), box)\n",
    "            img = transform(img).unsqueeze(0)\n",
    "            output = classifier(img.to(device))\n",
    "            print(output.detach().cpu().tolist())\n",
    "            print(torch.max(output))\n",
    "            predLabels.append(torch.max(output, 1)[1].data.squeeze().item())\n",
    "            predScores.append(torch.max(output, 1)[0].data.squeeze().item())\n",
    "            boxesKeep.append(box.detach().cpu().tolist())\n",
    "            \n",
    "        print(output)\n",
    "        boxesKeep = torch.tensor(boxesKeep, dtype=torch.float32)\n",
    "        print(predLabels)\n",
    "        print()\n",
    "        print(predScores)\n",
    "        break\n",
    "             \n",
    "        # test_output = classifier(images)\n",
    "        # pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "        # accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "        # predictions.extend(pred_y.cpu().numpy())\n",
    "        # actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "#     # During evaluation, filter out predictions below a certain probability threshold\n",
    "#     bbox = bbox_output.view(-1, self.max_objects, 4)\n",
    "#     probabilities = F.softmax(classifier, dim=2)\n",
    "#     # Apply thresholding to filter out predictions with low confidence\n",
    "#     thresholded_probabilities, indices = torch.max(probabilities, dim=2)\n",
    "#     mask = thresholded_probabilities > 0.5\n",
    "#     classifier = classifier[mask]\n",
    "#     bbox = bbox[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Testing Data')\n",
    "plot_sample(images.detach().cpu(), predLabels, boxesKeep, num.item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Testing Data')\n",
    "plot_sample(images.detach().cpu(), labels.tolist(), bboxes.tolist(), num.item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Imagery of Model Output\n",
    "\n",
    "### Training Images Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainImages, trainLabels, trainImagesFlipped = next(iter(trainLoader))\n",
    "# samples = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs, __ = model(trainImages.to(device))\n",
    "\n",
    "# j = 0\n",
    "# for i, label in enumerate(trainLabels):\n",
    "#     if label.item() in samples:\n",
    "#         f = plt.figure(figsize=(12, 4))      \n",
    "#         ax1 = f.add_subplot(131)\n",
    "#         ax1.imshow(trainImages[i].squeeze(), cmap='gray')\n",
    "#         ax1.axis('off')\n",
    "#         ax1.set_title(f'Original - {label.item()}')\n",
    "#         ax2 = f.add_subplot(132)\n",
    "#         ax2.imshow(outputs[i].detach().cpu()[0].squeeze(), cmap='gray')\n",
    "#         ax2.axis('off')\n",
    "#         ax2.set_title(f'Reconstruction (Flipped) - {label.item()}')\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "        \n",
    "#         samples.remove(label.item())\n",
    "#         j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Images Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testImages, testLabels, testImagesFlipped = next(iter(testLoader))\n",
    "# samples = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs, __ = model(testImages.to(device))\n",
    "\n",
    "# j = 0\n",
    "# for i, label in enumerate(testLabels):\n",
    "#     if label.item() in samples:\n",
    "#         f = plt.figure(figsize=(12, 4))      \n",
    "#         ax1 = f.add_subplot(131)\n",
    "#         ax1.imshow(testImages[i].squeeze(), cmap='gray')\n",
    "#         ax1.axis('off')\n",
    "#         ax1.set_title(f'Original - {label.item()}')\n",
    "#         ax2 = f.add_subplot(132)\n",
    "#         ax2.imshow(outputs[i].detach().cpu()[0].squeeze(), cmap='gray')\n",
    "#         ax2.axis('off')\n",
    "#         ax2.set_title(f'Reconstruction (Flipped) - {label.item()}')\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "        \n",
    "#         samples.remove(label.item())\n",
    "#         j += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
