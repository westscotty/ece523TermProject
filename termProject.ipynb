{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Project\n",
    "\n",
    "Team: Rain Price, Weston Scott\n",
    "\n",
    "ECE 523 | Engineering Applications of Machine Learning and Data Analytics\n",
    "\n",
    "Professor Abhijit Mahalanobis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle and People Detection with the FLIR Thermal Dataset\n",
    "\n",
    "![alt text](problemStatement.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Get simple training model working\n",
    "- Get resulting images and predictions showing\n",
    "- Run all images through model and check error\n",
    "- Rewrite Resnet layers to be our own homegrown solution?\n",
    "- Survive this class ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from PIL import Image, ImageFilter\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xml.etree.ElementTree as ET\n",
    "from torchvision.transforms.functional import pad\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import ToTensor, transforms\n",
    "from copy import copy\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Seed, Device Architecture, and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "randomSeed = 2024\n",
    "np.random.seed(randomSeed)\n",
    "torch.manual_seed(randomSeed)\n",
    "\n",
    "print(f'PyTorch Version: {torch.__version__}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print(f'PyTorch Device: {device}')\n",
    "\n",
    "saveClassifierModel = False\n",
    "loadClassifierModel = True\n",
    "saveDetectionModel = True\n",
    "loadDetectionModel = False\n",
    "classifierModelPath = './classificationModel.pt'\n",
    "detectorModelPath = './DetectionModel.pt'\n",
    "numWorkers = 2\n",
    "learnRate = 0.001\n",
    "batchSizeClass = 64\n",
    "batchSizeDetect = 1\n",
    "maxEpochs = 10\n",
    "normalizeImages = True\n",
    "\n",
    "trackingLabels = ['person', 'car']\n",
    "colors = ['r', 'g']\n",
    "maxObjects = 83\n",
    "numClasses = len(trackingLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to see if input images are the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/images_thermal_train/data'\n",
    "\n",
    "imgs = os.listdir(path)\n",
    "sizes = []\n",
    "for img in imgs:\n",
    "    filename = os.path.join(path, img)\n",
    "    image = Image.open(filename)\n",
    "    sizes.append(image.size)\n",
    "print(sizes[-1])\n",
    "np.unique(sizes, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training/ Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = './data/images_thermal_train'\n",
    "valPath = './data/images_thermal_val'\n",
    "testPath = './data/video_thermal_test'\n",
    "dataDir = 'data'\n",
    "jsonFile = 'coco.json'\n",
    "trimmedJsonFile = 'trimmed_coco.json'\n",
    "\n",
    "jsonFiles = { \n",
    "              'train' : os.path.join(trainPath, jsonFile),\n",
    "              'val' : os.path.join(valPath, jsonFile),\n",
    "              'test' : os.path.join(testPath, jsonFile)\n",
    "            }\n",
    "\n",
    "jsonFilesTrimmed = { \n",
    "              'train' : os.path.join(trainPath, trimmedJsonFile),\n",
    "              'val' : os.path.join(valPath, trimmedJsonFile),\n",
    "              'test' : os.path.join(testPath, trimmedJsonFile)\n",
    "            }\n",
    "\n",
    "imagePaths = { \n",
    "              'train' : trainPath,\n",
    "              'val' : valPath,\n",
    "              'test' : testPath\n",
    "            }\n",
    "\n",
    "for key, val in jsonFiles.items():\n",
    "    if os.path.isfile(val):\n",
    "        print(f'coco.json Exists: {key}, {val}')\n",
    "    \n",
    "for key, val in imagePaths.items():\n",
    "    if os.path.isdir(val):\n",
    "        print(f'Data Directory Exists: {key}, {val}')\n",
    "        \n",
    "labelMap = {\n",
    "            1:  'person',\n",
    "            2:  'bike', #(renamed from \"bicycle\")\n",
    "            3:  'car', #(this includes pick-up trucks and vans)\n",
    "            4:  'motor', #(renamed from \"motorcycle\" for brevity)\n",
    "            6:  'bus',\n",
    "            7:  'train',\n",
    "            8:  'truck', #(semi/freight truck, excluding pickup truck)\n",
    "            10: 'light', #(renamed from \"traffic light\" for brevity)\n",
    "            11: 'hydrant', #(renamed \"fire hydrant\" for brevity)\n",
    "            12: 'sign', #(renamed from \"street sign\" for brevity)\n",
    "            17: 'dog',\n",
    "            18: 'deer',\n",
    "            37: 'skateboard',\n",
    "            73: 'stroller', #(four-wheeled carriage for a child, also called pram)\n",
    "            75: 'scooter',\n",
    "            79: 'other vehicle' #(less common vehicles like construction equipment and trailers)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxMaxWidth = 1\n",
    "boxMaxHeight = 1\n",
    "for key, val in jsonFiles.items():\n",
    "    \n",
    "    print(f\"Generating trimmed coco.json files from: {val} for {key} data at: {jsonFilesTrimmed[key]} ...\")\n",
    "    with open(val, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "    imageIDs = []\n",
    "    for annot in data['annotations']:\n",
    "        if labelMap[annot['category_id']] in trackingLabels:\n",
    "            annotations.append(annot)\n",
    "            imageIDs.append(annot['image_id'])\n",
    "            \n",
    "            bbox = annot['bbox']\n",
    "            if bbox[2] > boxMaxWidth:\n",
    "                boxMaxWidth = bbox[2]\n",
    "            if bbox[3] > boxMaxHeight:\n",
    "                boxMaxHeight = bbox[3]\n",
    "                \n",
    "    imageIDs= np.unique(imageIDs)\n",
    "    \n",
    "    for i, image in enumerate(data['images']):\n",
    "        if (image['id'] in imageIDs):\n",
    "            images.append(image)\n",
    "    \n",
    "    trimmedData = { 'annotations': annotations,\n",
    "                    'images': images,\n",
    "                    'ids': dict(enumerate(trackingLabels))}\n",
    "    \n",
    "    with open(jsonFilesTrimmed[key], \"w\") as outfile: \n",
    "        json.dump(trimmedData, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxMaxWidth =  int(boxMaxWidth * 640/image['width'])\n",
    "boxMaxHeight = int(boxMaxHeight * 640/image['height'])\n",
    "\n",
    "print(f'Maximum Object Box Width: {boxMaxWidth}')\n",
    "print(f'Maximum Object Box Height: {boxMaxHeight}')\n",
    "print(f'Maximum Number of Objects: {maxObjects}')\n",
    "print(f'Number of Classes: {numClasses}')\n",
    "\n",
    "del data, images, annotations, imageIDs, trimmedData, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizeBoxImage(img, bbox):\n",
    "    x, y, w, h = bbox\n",
    "    x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "    img = img[y:y+h, x:x+w]\n",
    "    if w <= 80 and h <= 80:\n",
    "        img = cv2.resize(img, (80, 80))\n",
    "    elif w <= 160 and h <= 160:\n",
    "        img = cv2.resize(img, (160, 160))\n",
    "        img = cv2.pyrDown(img)\n",
    "    if w <= 320 and h <= 320:\n",
    "        img = cv2.resize(img, (320, 320))\n",
    "        img = cv2.pyrDown(img)\n",
    "        img = cv2.pyrDown(img)\n",
    "    else:\n",
    "        img = cv2.resize(img, (640, 640))\n",
    "        img = cv2.pyrDown(img)\n",
    "        img = cv2.pyrDown(img)\n",
    "        img = cv2.pyrDown(img) ## final image size is 80x80\n",
    "    return img\n",
    "\n",
    "class ThermalCocoDataset(Dataset):\n",
    "    def __init__(self, jsonFile, imageDir, trackingLabels, labelMap, maxWidth=2, maxHeight=2, singleObject=False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            jsonFile (str): Path to the COCO-style JSON file containing annotations.\n",
    "            imageDir (str): Directory containing the images.\n",
    "            tracking_labels (list): List of tracking labels.\n",
    "            label_map (dict): Mapping of category IDs to labels.\n",
    "            single_object (bool): Whether to treat each image as containing a single object.\n",
    "            transform (callable, optional): Optional transform to be applied to the images.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.jsonFile = jsonFile\n",
    "        self.imageDir = imageDir\n",
    "        self.transform = transform\n",
    "        self.trackingLabels = trackingLabels\n",
    "        self.labelMap = labelMap\n",
    "        self.maxWidth = maxWidth\n",
    "        self.maxHeight = maxHeight\n",
    "        self.singleObject = singleObject\n",
    "        self._load_json()\n",
    "\n",
    "    def _load_json(self):\n",
    "        \"\"\"Load JSON annotations.\"\"\"\n",
    "        \n",
    "        with open(self.jsonFile, 'r') as f:\n",
    "            data = json.load(f)                    \n",
    "        \n",
    "        self.annotations = data['annotations']\n",
    "        self.images = data['images']\n",
    "            \n",
    "    def _adjust_bounding_box(self, bbox, width, height):\n",
    "        \"\"\"Adjust bounding box coordinates to match resized image.\"\"\"\n",
    "        return [bbox[0] * (640 / width),\n",
    "                bbox[1] * (640 / height),\n",
    "                bbox[2] * (640 / width),\n",
    "                bbox[3] * (640 / height)]\n",
    "    \n",
    "    def _get_single_object(self, idx):\n",
    "        \"\"\"Get image, label, bounding box, and number of objects for single object mode.\"\"\"\n",
    "        \n",
    "        annotation = self.annotations[idx]\n",
    "        image_id = annotation['image_id']\n",
    "        for entry in self.images:\n",
    "            if entry['id'] == image_id:\n",
    "                image_file_name = entry['file_name']\n",
    "                width = entry['width']\n",
    "                height = entry['height']\n",
    "            \n",
    "        image_file = os.path.join(self.imageDir, f\"{image_file_name}\")\n",
    "        img = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "        img = cv2.resize(img, (640, 640))\n",
    "\n",
    "        if self.labelMap[annotation['category_id']] in self.trackingLabels:\n",
    "            # Assuming annotation format: [x, y, width, height]\n",
    "            bbox = self._adjust_bounding_box(annotation['bbox'], width, height)\n",
    "            tmpLabel = self.labelMap[annotation['category_id']]\n",
    "            \n",
    "        img = sizeBoxImage(img, bbox)\n",
    "        label = torch.tensor(self.trackingLabels.index(tmpLabel))\n",
    "        bbox = torch.tensor(bbox)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label, bbox, torch.tensor(1)\n",
    "\n",
    "    def _get_multi_objects(self, idx):\n",
    "        \"\"\"Get image, labels, bounding boxes, and number of objects for multi-object mode.\"\"\"\n",
    "        \n",
    "        image = self.images[idx]\n",
    "        image_file = os.path.join(self.imageDir, f\"{image['file_name']}\")\n",
    "        img = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "        img = cv2.resize(img, (640, 640))\n",
    "            \n",
    "        id = image['id']\n",
    "        width = image['width']\n",
    "        height = image['height']\n",
    "        \n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        for entry in self.annotations:\n",
    "            if entry['image_id'] == id:\n",
    "                if (self.labelMap[entry['category_id']] in self.trackingLabels):\n",
    "                    tmpLabel = self.labelMap[entry['category_id']]\n",
    "                    bboxes.append(self._adjust_bounding_box(entry['bbox'], width, height))\n",
    "                    labels.append(self.trackingLabels.index(tmpLabel))\n",
    "                    \n",
    "        numObjects = torch.tensor(len(labels))\n",
    "        labels = torch.tensor(labels).squeeze()\n",
    "        bboxes = torch.tensor(bboxes, dtype=torch.float32)\n",
    "    \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, labels, bboxes, numObjects\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of images in the dataset.\"\"\"\n",
    "        \n",
    "        if self.singleObject:\n",
    "            return len(self.annotations)\n",
    "        else:\n",
    "            return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get the image, labels, and bounding boxes for the given index.\"\"\"\n",
    "        \n",
    "        if self.singleObject:\n",
    "            return self._get_single_object(idx)\n",
    "        else:\n",
    "            return self._get_multi_objects(idx)\n",
    "\n",
    "if normalizeImages:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize image\n",
    "    ])\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataClass = ThermalCocoDataset(jsonFilesTrimmed['train'], imagePaths['train'], trackingLabels, labelMap, 640, 640, True, transform=transform)\n",
    "valDataClass = ThermalCocoDataset(jsonFilesTrimmed['val'], imagePaths['val'], trackingLabels, labelMap, 640, 640, True, transform=transform)\n",
    "testDataClass = ThermalCocoDataset(jsonFilesTrimmed['test'], imagePaths['test'], trackingLabels, labelMap, 640, 640, True, transform=transform)\n",
    "\n",
    "trainDataDetect = ThermalCocoDataset(jsonFilesTrimmed['train'], imagePaths['train'], trackingLabels, labelMap, transform=transform)\n",
    "valDataDetect = ThermalCocoDataset(jsonFilesTrimmed['val'], imagePaths['val'], trackingLabels, labelMap, transform=transform)\n",
    "testDataDetect = ThermalCocoDataset(jsonFilesTrimmed['test'], imagePaths['test'], trackingLabels, labelMap, transform=transform)\n",
    "\n",
    "trainLoaderClass = DataLoader(trainDataClass, batch_size=batchSizeClass, shuffle=True, num_workers=numWorkers)#, collate_fn=custom_collate_fn)\n",
    "valLoaderClass = DataLoader(valDataClass, batch_size=batchSizeClass, shuffle=True, num_workers=numWorkers)\n",
    "testLoaderClass = DataLoader(testDataClass, batch_size=batchSizeClass, shuffle=False, num_workers=numWorkers)\n",
    "\n",
    "trainLoaderDetect = DataLoader(trainDataDetect, batch_size=batchSizeDetect, shuffle=False, num_workers=numWorkers)#, collate_fn=custom_collate_fn)\n",
    "valLoaderDetect = DataLoader(valDataDetect, batch_size=batchSizeDetect, shuffle=False, num_workers=numWorkers)\n",
    "testLoaderDetect = DataLoader(testDataDetect, batch_size=1, shuffle=False, num_workers=numWorkers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corner_rect(bb, color='red'):\n",
    "    bb = np.array(bb, dtype=np.float32)\n",
    "    return plt.Rectangle((bb[0], bb[1]), bb[2], bb[3], color=color,\n",
    "                         fill=False, lw=1)\n",
    "\n",
    "def show_corner_bb(im, bb, c=None, cLabel='', color='red', createFig=False):\n",
    "    if createFig:\n",
    "        plt.figure(figsize=(6,6))\n",
    "        if not cLabel == '':\n",
    "            plt.title(f'{cLabel} Class: {c}')\n",
    "    plt.imshow(im.squeeze(), cmap=plt.cm.gray)\n",
    "    plt.gca().add_patch(create_corner_rect(bb, color=color))\n",
    "    \n",
    "def plot_sample(image, labels, bboxes, num, showbb=True):\n",
    "    \n",
    "    plt.imshow(image.squeeze(), cmap=\"gray\")  # Convert (C, H, W) tensor to (H, W, C) for plotting\n",
    "    if showbb:\n",
    "        plt.title(f'Number of Objects: {num}')\n",
    "        try:\n",
    "            for bbox, label in zip(bboxes, labels):\n",
    "                x, y, w, h = bbox\n",
    "                plt.gca().add_patch(plt.Rectangle((x, y), w, h, linewidth=1, edgecolor=colors[label], facecolor='none'))\n",
    "                plt.text(x, y-5, f'{trackingLabels[label]}', color=colors[label])\n",
    "        except:\n",
    "            try:\n",
    "                x, y, w, h = bboxes\n",
    "            except:\n",
    "                x, y, w, h = bboxes[0]\n",
    "            plt.gca().add_patch(plt.Rectangle((x, y), w, h, linewidth=1, edgecolor=colors[labels], facecolor='none'))\n",
    "            plt.text(x, y-5, f'{trackingLabels[labels]}', color=colors[labels])\n",
    "        plt.axis('off')\n",
    "    else:\n",
    "        plt.title(f'Sample {trackingLabels[labels]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Imagery From Training Data\n",
    "\n",
    "### Original Images (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Training Data Sample')\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(trainDataClass), size=(1,)).item()\n",
    "    image, labels, bboxes, num = trainDataClass[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), num.item(), showbb=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Training Data Sample')\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(trainDataDetect), size=(1,)).item()\n",
    "    image, labels, bboxes, num = trainDataDetect[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), num.item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Images (Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Training Data Sample')\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(valDataClass), size=(1,)).item()\n",
    "    image, labels, bboxes, num = valDataClass[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), num.item(), showbb=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Validation Data Sample')\n",
    "\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(valDataDetect), size=(1,)).item()\n",
    "    image, labels, bboxes, num = valDataDetect[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), num.item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Images (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Training Data Sample')\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(testDataClass), size=(1,)).item()\n",
    "    image, labels, bboxes, num = testDataClass[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), num.item(), showbb=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Testing Data Sample')\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    idx = torch.randint(len(testDataDetect), size=(1,)).item()\n",
    "    image, labels, bboxes, num = testDataDetect[idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plot_sample(image, labels.tolist(), bboxes.tolist(), num.item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        inplace = False\n",
    "        self.batchNorm = nn.BatchNorm2d(1)\n",
    "        resnet = models.resnet18(weights=None)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        layers = list(resnet.children())[:6]\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Linear(128, 32), \n",
    "                                         nn.ReLU(inplace=inplace), \n",
    "                                         nn.Linear(32, num_classes),\n",
    "                                         nn.Softmax(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batchNorm(x)\n",
    "        x = self.features(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        classifier_output = self.classifier(x)\n",
    "        return classifier_output\n",
    "    \n",
    "classifier = Classifier(numClasses)\n",
    "\n",
    "classifier = classifier.cpu()\n",
    "\n",
    "if str(device) == 'cuda':\n",
    "    model = classifier.to(device)\n",
    "\n",
    "summary(classifier, (1,80,80))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterionClassifier = nn.CrossEntropyLoss()   \n",
    "print(f'Criterion: {criterionClassifier}')\n",
    "\n",
    "optimizerClassifier = optim.Adam(model.parameters(), lr = learnRate)   \n",
    "print(f'\\nOptimizer: {optimizerClassifier}')\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizerClassifier, mode='min')\n",
    "print(f'\\nscheduler: {scheduler}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loadClassifierModel:\n",
    "    classifier = Classifier(numClasses)\n",
    "    # optimizerClassifier = optimizerClassifier\n",
    "    checkpoint = torch.load(classifierModelPath)\n",
    "    classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizerClassifier.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    criterionClassifier = checkpoint['loss']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.to(device)\n",
    "criterionClassifier = criterionClassifier.to(device)\n",
    "\n",
    "if not loadClassifierModel:\n",
    "        \n",
    "    trainLoss = []\n",
    "    trainAcc = []\n",
    "    testLoss = []\n",
    "    testAcc = []\n",
    "\n",
    "    for epoch in range(1, maxEpochs+1):\n",
    "        epochLoss = []\n",
    "        epochAcc = []\n",
    "        testEpochLoss = []\n",
    "        testEpochAcc = []\n",
    "        classifier.train()\n",
    "\n",
    "        for i, (images, labels, __, __) in tqdm(enumerate(trainLoaderClass)):\n",
    "            optimizerClassifier.zero_grad()\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            loss = criterionClassifier(outputs, labels)\n",
    "            loss.backward()          \n",
    "            optimizerClassifier.step()\n",
    "            lossVal = loss.item()\n",
    "            \n",
    "            pred = torch.max(outputs, 1)[1].data.squeeze()\n",
    "            accuracy = (pred == labels).sum().item() / float(labels.size(0))\n",
    "\n",
    "            epochLoss.append(float(lossVal))\n",
    "            epochAcc.append(float(accuracy))\n",
    "                \n",
    "        trainLoss.append(np.mean(epochLoss))\n",
    "        trainAcc.append(np.mean(epochAcc))\n",
    "        # scheduler.step(trainLoss[-1])\n",
    "                \n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            for testImages, testLabels, __, __ in tqdm(valLoaderClass):\n",
    "                testImages = testImages.to(device)\n",
    "                testLabels = testLabels.to(device)\n",
    "                    \n",
    "                testOutput = classifier(testImages)\n",
    "                lossVal = criterionClassifier(testOutput, testLabels)\n",
    "            \n",
    "                if str(device) == 'cuda':\n",
    "                    lossVal = lossVal.cpu()\n",
    "                    \n",
    "                predVal = torch.max(testOutput, 1)[1].data.squeeze()\n",
    "                accuracyVal = (predVal == testLabels).sum().item() / float(testLabels.size(0))\n",
    "                \n",
    "                testEpochLoss.append(lossVal)\n",
    "                testEpochAcc.append(accuracyVal)\n",
    "                \n",
    "            testLoss.append(np.mean(testEpochLoss))\n",
    "            testAcc.append(np.mean(testEpochAcc))\n",
    "                \n",
    "        print(f'[Epoch: {epoch}/{maxEpochs}] Loss: {np.round(trainLoss[-1], 5)}')\n",
    "\n",
    "        if saveClassifierModel and epoch % 1 == 0: ## save model every 5th epoch\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizerClassifier.state_dict(),\n",
    "                    'loss': trainLoss[-1],\n",
    "                    }, classifierModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if saveClassifierModel and not loadClassifierModel:\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizerClassifier.state_dict(),\n",
    "            'loss': trainLoss[-1],\n",
    "            }, classifierModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not loadClassifierModel:\n",
    "    print(f'Final MSE ({maxEpochs} epochs): {trainLoss[-1]}\\n')\n",
    "    \n",
    "    f = plt.figure(figsize=(10,8))\n",
    "    plt.plot(trainLoss, label=\"train\")\n",
    "    plt.plot(testLoss, label=\"test\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"cross entropy\")\n",
    "    plt.title(\"Epochs vs. Loss Function\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    f = plt.figure(figsize=(10,8))\n",
    "    plt.plot(trainAcc, label=\"train\")\n",
    "    plt.plot(testAcc, label=\"test\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(\"Epochs vs. Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "actuals = []\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels, __, __ in tqdm(testLoaderClass):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "            \n",
    "        test_output = classifier(images)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "        accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "        predictions.extend(pred_y.cpu().numpy())\n",
    "        actuals.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorCount = [predictions[i] == actuals[i] for i in range(len(actuals))]\n",
    "print(f\"\\nCorrect classifications on {len(predictions)} images: {np.sum(errorCount)}/{len(predictions)} | {np.sum(errorCount)/len(predictions)*100}%\")\n",
    "\n",
    "## Create Confusion matrix\n",
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "confusion_matrix = metrics.confusion_matrix(actuals, predictions)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels=trackingLabels)\n",
    "\n",
    "cm_display.plot(ax=ax)\n",
    "plt.title(\"Confusion Matrix for Test Data\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions).reshape(len(predictions), 1)\n",
    "actuals = np.array(actuals).reshape(len(actuals), 1)\n",
    "bidxs = np.where(actuals != predictions)[0]\n",
    "gidxs = np.where(actuals == predictions)[0]\n",
    "# print(f'Incorrect Classification Indices:\\n{bidxs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Correct Classifications: (Actual - Predicted)')\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(gidxs), size=(1,)).item()\n",
    "    sample_idx = gidxs[sample_idx]\n",
    "    img, label, __, __ = testDataClass[sample_idx]\n",
    "    pred = predictions[sample_idx]\n",
    "    act = actuals[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(f'{trackingLabels[label]} - {trackingLabels[pred[0]]}')\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle('Incorrect Classifications: (Actual - Predicted)')\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(bidxs), size=(1,)).item()\n",
    "    sample_idx = bidxs[sample_idx]\n",
    "    img, label, __, __ = testDataClass[sample_idx]\n",
    "    pred = predictions[sample_idx]\n",
    "    act = actuals[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(f'{trackingLabels[label]} - {trackingLabels[pred[0]]}')\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDetection(nn.Module):\n",
    "    def __init__(self, num_classes, max_objects):\n",
    "        super(ClassificationDetection, self).__init__()\n",
    "        inplace = False\n",
    "        resnet = models.resnet50(weights=None)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        layers = list(resnet.children())[:6]\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.max_objects = max_objects\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Linear(512, 64), \n",
    "                                         nn.ReLU(inplace=inplace), \n",
    "                                         nn.Linear(64, num_classes*max_objects))\n",
    "        self.bb = nn.Sequential(nn.Linear(512, 64),\n",
    "                                nn.ReLU(inplace=inplace),\n",
    "                                nn.Linear(64, 4*max_objects))\n",
    "        \n",
    "    def forward(self, x, num_available_objects=1):\n",
    "        x = self.features(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        classifier_output = self.classifier(x)\n",
    "        bbox_output = self.bb(x)\n",
    "        \n",
    "        if self.training:\n",
    "            # During training, return predictions for the specified number of objects\n",
    "            classifier = classifier_output[:, :self.num_classes*num_available_objects]\n",
    "            bbox = bbox_output[:, :4*num_available_objects]\n",
    "        else:\n",
    "            # During evaluation, filter out predictions below a certain probability threshold\n",
    "            classifier = classifier_output.view(-1, self.max_objects, self.num_classes)\n",
    "            bbox = bbox_output.view(-1, self.max_objects, 4)\n",
    "            probabilities = F.softmax(classifier, dim=2)\n",
    "            # Apply thresholding to filter out predictions with low confidence\n",
    "            thresholded_probabilities, indices = torch.max(probabilities, dim=2)\n",
    "            mask = thresholded_probabilities > 0.5\n",
    "            classifier = classifier[mask]\n",
    "            bbox = bbox[mask]\n",
    "\n",
    "        return classifier, bbox\n",
    "    \n",
    "model = ClassificationDetection(numClasses, maxObjects)\n",
    "\n",
    "model = model.cpu()\n",
    "\n",
    "if str(device) == 'cuda':\n",
    "    model = model.to(device)\n",
    "\n",
    "summary(model, (1,640,640))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Criterion and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DetectionLoss, self).__init__()\n",
    "        self.cls_loss = nn.CrossEntropyLoss() ## Classifier\n",
    "        self.reg_loss = nn.SmoothL1Loss() ## Bounding Box\n",
    "\n",
    "    def forward(self, pred_cls, pred_reg, target_cls, target_reg):\n",
    "        pred_cls = torch.flatten(pred_cls)\n",
    "        pred_reg = torch.flatten(pred_reg)\n",
    "        target_cls = torch.flatten(target_cls)\n",
    "        target_reg = torch.flatten(target_reg)\n",
    "        classification_loss = self.cls_loss(pred_cls, target_cls)\n",
    "        regression_loss = self.reg_loss(pred_reg, target_reg)\n",
    "        return classification_loss + regression_loss\n",
    "    \n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=learnRate)\n",
    "print(f'Optimizer: {optimizer}')\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Adjust the scheduler parameters\n",
    "print(f'Scheduler: {scheduler}')\n",
    "\n",
    "# Define the loss function\n",
    "criterion = DetectionLoss()\n",
    "print(f'\\nCriterion: {criterion}')\n",
    "\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model (If Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loadModel:\n",
    "    model = ClassificationDetection(len(labelMap), len(labelMap))\n",
    "    optimizer = optimizer\n",
    "    checkpoint = torch.load(modelPath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    criterion = checkpoint['loss']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not loadModel:\n",
    "    trainLoss = []\n",
    "\n",
    "    for epoch in range(1, maxEpochs+1):\n",
    "        epochLoss = []\n",
    "        testEpochLoss = []\n",
    "        model.train()\n",
    "\n",
    "        for i, (images, labels, bboxes, num) in enumerate(trainLoader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if str(device) == 'cuda':\n",
    "                images = images.to(device)\n",
    "                bboxes = bboxes.to(device)\n",
    "                labels = labels.to(device) \n",
    "                num = num.to(device)\n",
    "            \n",
    "            predLabels, predBoxes = model(images, num)\n",
    "            predLabelsMax = torch.max(predLabels.view(num, numClasses), 1)[1]\n",
    "            predBoxesMod = predBoxes.view(-1, num, 4)\n",
    "            # print(predLabelsMax)\n",
    "            # print(labels)\n",
    "            loss = criterion(predLabelsMax.float(), predBoxesMod, labels.float(), bboxes)\n",
    "            loss.backward()          \n",
    "            optimizer.step()\n",
    "            lossVal = loss.item()\n",
    "            epochLoss.append(float(lossVal))\n",
    "\n",
    "        trainLoss.append(np.mean(epochLoss))\n",
    "        # scheduler.step(trainLoss[-1])\n",
    "                \n",
    "        print(f'[Epoch: {epoch}/{maxEpochs}] Loss: {np.round(trainLoss[-1], 5)}')\n",
    "\n",
    "        if saveModel and epoch % 1 == 0: ## save model every 5th epoch\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                    }, modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model (If Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if saveModel and not loadModel:\n",
    "#     torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': criterion,\n",
    "#             'loss_bbox': criterion_bbox,\n",
    "#             }, modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not loadModel:\n",
    "    print(f'Final MSE ({maxEpochs} epochs): {trainLoss[-1]}\\n')\n",
    "    \n",
    "    f = plt.figure(figsize=(10,8))\n",
    "    plt.plot(trainLoss, label=\"train\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"cross entropy\")\n",
    "    plt.title(\"Epochs vs. Loss Function\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Imagery of Model Output\n",
    "\n",
    "### Training Images Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainImages, trainLabels, trainImagesFlipped = next(iter(trainLoader))\n",
    "# samples = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs, __ = model(trainImages.to(device))\n",
    "\n",
    "# j = 0\n",
    "# for i, label in enumerate(trainLabels):\n",
    "#     if label.item() in samples:\n",
    "#         f = plt.figure(figsize=(12, 4))      \n",
    "#         ax1 = f.add_subplot(131)\n",
    "#         ax1.imshow(trainImages[i].squeeze(), cmap='gray')\n",
    "#         ax1.axis('off')\n",
    "#         ax1.set_title(f'Original - {label.item()}')\n",
    "#         ax2 = f.add_subplot(132)\n",
    "#         ax2.imshow(outputs[i].detach().cpu()[0].squeeze(), cmap='gray')\n",
    "#         ax2.axis('off')\n",
    "#         ax2.set_title(f'Reconstruction (Flipped) - {label.item()}')\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "        \n",
    "#         samples.remove(label.item())\n",
    "#         j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Images Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testImages, testLabels, testImagesFlipped = next(iter(testLoader))\n",
    "# samples = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs, __ = model(testImages.to(device))\n",
    "\n",
    "# j = 0\n",
    "# for i, label in enumerate(testLabels):\n",
    "#     if label.item() in samples:\n",
    "#         f = plt.figure(figsize=(12, 4))      \n",
    "#         ax1 = f.add_subplot(131)\n",
    "#         ax1.imshow(testImages[i].squeeze(), cmap='gray')\n",
    "#         ax1.axis('off')\n",
    "#         ax1.set_title(f'Original - {label.item()}')\n",
    "#         ax2 = f.add_subplot(132)\n",
    "#         ax2.imshow(outputs[i].detach().cpu()[0].squeeze(), cmap='gray')\n",
    "#         ax2.axis('off')\n",
    "#         ax2.set_title(f'Reconstruction (Flipped) - {label.item()}')\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "        \n",
    "#         samples.remove(label.item())\n",
    "#         j += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
